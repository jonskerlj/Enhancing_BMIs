{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to test different classifiers on the CO task data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyaldata import *\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import WienerFilterClassification\n",
    "from Neural_Decoding.decoders import SVClassification\n",
    "from Neural_Decoding.decoders import DenseNNClassification\n",
    "from Neural_Decoding.decoders import LSTMClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = '../raw_data/'\n",
    "fname = os.path.join(data_dir, \"Chewie_CO_CS_2016-10-14.mat\")\n",
    "\n",
    "# load TrialData .mat file into a DataFrame\n",
    "df = mat2dataframe(fname, shift_idx_fields=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig: (119, 88)\n",
      "New: (40, 88)\n",
      "Orig: (40, 88)\n",
      "New: (40, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "# Keep only successful trials\n",
    "df = select_trials(df, \"result == 'R'\")\n",
    "\n",
    "# Preprocessing\n",
    "# combine time bins into longer ones, e.g. group 3 time bins together\n",
    "td = combine_time_bins(df, 3)\n",
    "\n",
    "# Obtain only the interval between idx_target_on and idx_go_cue\n",
    "print('Orig:', td.M1_spikes[0].shape)\n",
    "td = restrict_to_interval(td, start_point_name='idx_target_on', end_point_name='idx_go_cue')\n",
    "print('New:', td.M1_spikes[0].shape)\n",
    "\n",
    "# Remove low-firing neurons\n",
    "print('Orig:', td.M1_spikes[0].shape)\n",
    "td = remove_low_firing_neurons(td, \"M1_spikes\",  5)\n",
    "td = remove_low_firing_neurons(td, \"PMd_spikes\", 5)\n",
    "print('New:', td.M1_spikes[0].shape)\n",
    "\n",
    "# Combine M1 and PMd\n",
    "td = merge_signals(td, [\"M1_spikes\", \"PMd_spikes\"], \"both_spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing subsets\n",
    "# total number of trials\n",
    "N = td.shape[0]\n",
    "\n",
    "#Number of M1_neurons\n",
    "N_M1 = td.M1_spikes[0].shape[1]\n",
    "#Number of PMd_neurons\n",
    "N_PMd = td.PMd_spikes[0].shape[1]\n",
    "\n",
    "M1_spikes = np.empty([N_M1,N])\n",
    "PMd_spikes = np.empty([N_PMd,N])\n",
    "y = np.empty([N,1])\n",
    "\n",
    "for i in range(N):\n",
    "    # Get the neuron spikes for a given trial in train data\n",
    "    M1_trial = np.transpose(td.M1_spikes[i])\n",
    "    PMd_trial = np.transpose(td.PMd_spikes[i])\n",
    "    \n",
    "    # Sum all the spikes in the given trial and save them\n",
    "    M1_spikes[:,i] = np.sum(M1_trial, axis=1)\n",
    "    PMd_spikes[:,i] = np.sum(PMd_trial, axis=1)\n",
    "    \n",
    "    # Get the label\n",
    "    y[i] = determine_angle(td.target_direction[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feature vector\n",
    "F_M1 = np.empty([N, N_M1])\n",
    "F_PMd = np.empty([N, N_PMd])\n",
    "for i in range(N):#in range(M1_spikes.shape[1]):\n",
    "    total_M1_spikes = np.sum(M1_spikes[:,i]);\n",
    "    total_PMd_spikes = np.sum(PMd_spikes[:,i])\n",
    "    \n",
    "    f_M1 = np.transpose(M1_spikes[:,i])/total_M1_spikes\n",
    "    f_PMd = np.transpose(PMd_spikes[:,i])/total_PMd_spikes\n",
    "    \n",
    "    # Store average firing rates\n",
    "    F_M1[i,:] = f_M1\n",
    "    F_PMd[i,:] = f_PMd\n",
    "    \n",
    "# Additional combined feature vector\n",
    "F_M1_PMd = np.concatenate((F_M1, F_PMd), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into test and train subsets\n",
    "split = int(0.8*N)\n",
    "\n",
    "\n",
    "y_train = y[0:split-1]\n",
    "y_test = y[split:]\n",
    "\n",
    "F_PMd_train = F_PMd[0:split-1,:]\n",
    "F_PMd_test = F_PMd[split:,:]\n",
    "\n",
    "F_M1_PMd_train = F_M1_PMd[0:split-1,:]\n",
    "F_M1_PMd_test = F_M1_PMd[split:,:]\n",
    "\n",
    "print(np.squeeze(y_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2635135135135135\n"
     ]
    }
   ],
   "source": [
    "## Train classifiers\n",
    "wf_classifier = WienerFilterClassification()\n",
    "\n",
    "wf_classifier.fit(F_M1_PMd_train, np.squeeze(y_train))\n",
    "\n",
    "wf_prediction = wf_classifier.predict(F_M1_PMd_test)\n",
    "\n",
    "check_wf = wf_prediction==np.squeeze(y_test)\n",
    "accuracy = np.count_nonzero(check_wf)/(y_test.shape[0])\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8445945945945946\n"
     ]
    }
   ],
   "source": [
    "## Train classifiers\n",
    "# Support vector classification\n",
    "sv_classifier = SVClassification()\n",
    "\n",
    "sv_classifier.fit(F_M1_PMd_train, np.squeeze(y_train))\n",
    "\n",
    "sv_prediction = sv_classifier.predict(F_M1_PMd_test)\n",
    "\n",
    "check_sv = sv_prediction==np.squeeze(y_test)\n",
    "accuracy = np.count_nonzero(check_sv)/(y_test.shape[0])\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3581081081081081\n"
     ]
    }
   ],
   "source": [
    "# DenseNN classifier\n",
    "dnn_classifier = DenseNNClassification()\n",
    "\n",
    "dnn_classifier.fit(F_PMd_train, np.squeeze(y_train))\n",
    "dnn_prediction = dnn_classifier.predict(F_PMd_test)\n",
    "\n",
    "check_dnn = dnn_prediction==np.squeeze(y_test)\n",
    "accuracy_dnn = np.count_nonzero(check_dnn)/(y_test.shape[0])\n",
    "print('Accuracy:', accuracy_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
