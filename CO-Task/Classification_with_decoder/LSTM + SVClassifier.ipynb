{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyaldata import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "\n",
    "#Import hyperparameter optimization packages\n",
    "try:\n",
    "    from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "except ImportError:\n",
    "    print(\"\\nWARNING: hyperopt package is not installed. You will be unable to use section 5.\")\n",
    "    pass\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import LSTMDecoder\n",
    "from Neural_Decoding.decoders import SVClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reassigns different angles into classes from 1 to 8\n",
    "# going anticlockwise starting from +x direction\n",
    "def determine_angle(angle):\n",
    "    if angle == 0:\n",
    "        return 1\n",
    "    elif angle == math.pi/4:\n",
    "        return 2\n",
    "    elif angle == math.pi/2:\n",
    "        return 3\n",
    "    elif angle == 3*math.pi/4:\n",
    "        return 4\n",
    "    elif angle == math.pi:\n",
    "        return 5\n",
    "    elif angle == -3*math.pi/4:\n",
    "        return 6\n",
    "    elif angle == -math.pi/2:\n",
    "        return 7\n",
    "    elif angle == -math.pi/4:\n",
    "        return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = '../raw_data/'\n",
    "fname = os.path.join(data_dir, \"Chewie_CO_CS_2016-10-14.mat\")\n",
    "\n",
    "# load TrialData .mat file into a DataFrame\n",
    "df = mat2dataframe(fname, shift_idx_fields=True)\n",
    "\n",
    "# Keep only successful trials\n",
    "df = select_trials(df, \"result == 'R'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "## Classification preprocessing\n",
    "\n",
    "# Preprocessing\n",
    "# combine time bins into longer ones, e.g. group 3 time bins together\n",
    "td_class = combine_time_bins(df, 3)\n",
    "\n",
    "# Obtain only the interval between idx_target_on and idx_go_cue\n",
    "td_class = restrict_to_interval(td_class, start_point_name='idx_target_on', end_point_name='idx_go_cue')\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_class = remove_low_firing_neurons(td_class, \"M1_spikes\",  5)\n",
    "td_class = remove_low_firing_neurons(td_class, \"PMd_spikes\", 5)\n",
    "\n",
    "# total number of trials\n",
    "N = td_class.shape[0]\n",
    "\n",
    "#Number of M1_neurons\n",
    "N_M1 = td_class.M1_spikes[0].shape[1]\n",
    "#Number of PMd_neurons\n",
    "N_PMd = td_class.PMd_spikes[0].shape[1]\n",
    "\n",
    "M1_spikes = np.empty([N_M1,N])\n",
    "PMd_spikes = np.empty([N_PMd,N])\n",
    "y = np.empty([N,1])\n",
    "\n",
    "for i in range(N):\n",
    "    # Get the neuron spikes for a given trial in train data\n",
    "    M1_trial = np.transpose(td_class.M1_spikes[i])\n",
    "    PMd_trial = np.transpose(td_class.PMd_spikes[i])\n",
    "    \n",
    "    # Sum all the spikes in the given trial and save them\n",
    "    M1_spikes[:,i] = np.sum(M1_trial, axis=1)\n",
    "    PMd_spikes[:,i] = np.sum(PMd_trial, axis=1)\n",
    "    \n",
    "    # Get the label\n",
    "    y[i] = determine_angle(td_class.target_direction[i])\n",
    "    \n",
    "# Build a feature vector\n",
    "F_M1 = np.empty([N, N_M1])\n",
    "F_PMd = np.empty([N, N_PMd])\n",
    "for i in range(N):#in range(M1_spikes.shape[1]):\n",
    "    total_M1_spikes = np.sum(M1_spikes[:,i]);\n",
    "    total_PMd_spikes = np.sum(PMd_spikes[:,i])\n",
    "    \n",
    "    f_M1 = np.transpose(M1_spikes[:,i])/total_M1_spikes\n",
    "    f_PMd = np.transpose(PMd_spikes[:,i])/total_PMd_spikes\n",
    "    \n",
    "    # Store average firing rates\n",
    "    F_M1[i,:] = f_M1\n",
    "    F_PMd[i,:] = f_PMd\n",
    "    \n",
    "# Combine M1 and PMd features\n",
    "F_M1_PMd = np.concatenate((F_M1, F_PMd), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train subsets\n",
    "split = int(0.8*N)\n",
    "\n",
    "y_train = y[0:split-1]\n",
    "y_test = y[split:]\n",
    "\n",
    "F_M1_PMd_train = F_M1_PMd[0:split-1,:]\n",
    "F_M1_PMd_test = F_M1_PMd[split:,:]\n",
    "\n",
    "\n",
    "## Train classifiers\n",
    "# Support vector classification\n",
    "sv_classifier = SVClassification()\n",
    "\n",
    "sv_classifier.fit(F_M1_PMd_train, np.squeeze(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [12:25<00:00, 93.17s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [12:13<00:00, 91.71s/it]\n"
     ]
    }
   ],
   "source": [
    "## Regression preprocessing\n",
    "folder='../preprocessed_data/'\n",
    "#ENTER THE FOLDER THAT YOUR DATA IS IN\n",
    "\n",
    "with open(folder+'individual_tar_data.pickle','rb') as f:\n",
    "    M1, M1_PMd,pos_binned,vels_binned,sizes,trial_len=pickle.load(f,encoding='latin1') #If using python 3\n",
    "\n",
    "neural_data = M1\n",
    "kinematics = [pos_binned, vels_binned] \n",
    "\n",
    "LSTM_models = []\n",
    "label_means = []\n",
    "x_flat_mean = []\n",
    "x_flat_std = []\n",
    "N_tar = 8\n",
    "for (idx,output) in enumerate(kinematics):\n",
    "    start = 0\n",
    "    end = sum(trial_len[0:sizes[0]])\n",
    "\n",
    "    # Loop over the data to obtain decoders for all 8 targets\n",
    "    for tar in tqdm(range(1, N_tar+1)): \n",
    "\n",
    "        # Preprocess data\n",
    "        bins_before=6 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        # Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "        # Function to get the covariate matrix that includes spike history from previous bins\n",
    "        X=get_spikes_with_history(neural_data[start:end],bins_before,bins_after,bins_current)\n",
    "\n",
    "        # Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "        #Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "        X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "        # Output covariates\n",
    "        #Set decoding output\n",
    "        y=output[start:end]\n",
    "\n",
    "        # Split into training / testing / validation sets\n",
    "        #Set what part of data should be part of the training/testing/validation sets\n",
    "        training_range=[0, 0.7]\n",
    "        testing_range=[0.7, 0.85]\n",
    "        valid_range=[0.85,1]\n",
    "\n",
    "        # Split data:\n",
    "        num_examples=X.shape[0]\n",
    "\n",
    "        #Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "        #This makes it so that the different sets don't include overlapping neural data\n",
    "        training_set=np.arange(int(np.round(training_range[0]*num_examples))+bins_before,int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "        testing_set=np.arange(int(np.round(testing_range[0]*num_examples))+bins_before,int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "        valid_set=np.arange(int(np.round(valid_range[0]*num_examples))+bins_before,int(np.round(valid_range[1]*num_examples))-bins_after)\n",
    "\n",
    "        #Get training data\n",
    "        X_train=X[training_set,:,:]\n",
    "        X_flat_train=X_flat[training_set,:]\n",
    "        y_train=y[training_set,:]\n",
    "\n",
    "        #Get testing data\n",
    "        X_test=X[testing_set,:,:]\n",
    "        X_flat_test=X_flat[testing_set,:]\n",
    "        y_test=y[testing_set,:]\n",
    "\n",
    "        #Get validation data\n",
    "        X_valid=X[valid_set,:,:]\n",
    "        X_flat_valid=X_flat[valid_set,:]\n",
    "        y_valid=y[valid_set,:]\n",
    "\n",
    "        # Process covariates\n",
    "        #Z-score \"X\" inputs. \n",
    "       # X_train_mean=np.nanmean(X_train,axis=0)\n",
    "       # X_train_std=np.nanstd(X_train,axis=0)\n",
    "       # X_train=(X_train-X_train_mean)/X_train_std\n",
    "        #X_test=(X_test-X_train_mean)/X_train_std\n",
    "       # X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "        #Z-score \"X_flat\" inputs. \n",
    "        #X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "       # X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "        #X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "        #X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "        #X_flat_valid=(X_flat_valid-X_flat_train_mean)/X_flat_train_std\n",
    "        \n",
    "        \n",
    "        #Zero-center outputs\n",
    "       # y_train_mean=np.mean(y_train,axis=0)\n",
    "       # y_train=y_train-y_train_mean\n",
    "        #y_test=y_test-y_train_mean\n",
    "       # y_valid=y_valid-y_train_mean\n",
    "\n",
    "       \n",
    "        \n",
    "        #Declare model\n",
    "        model_lstm=LSTMDecoder(units=470,dropout=0.13857,num_epochs=7)\n",
    "\n",
    "        #Fit model\n",
    "        model_lstm.fit(X_train,y_train)\n",
    "\n",
    "       \n",
    "\n",
    "        #R2_vw = r2_score(y_valid,y_valid_predicted_wf, multioutput='variance_weighted')\n",
    "\n",
    "        # Save the R2 value for a given neural data and kinematics\n",
    "        #R2[idx,tar-1] = R2_vw\n",
    "        LSTM_models.append(model_lstm)\n",
    "        #label_means.append(y_train_mean)\n",
    "        #x_flat_mean.append(X_flat_train_mean)\n",
    "        #x_flat_std.append(X_flat_train_std)\n",
    "        \n",
    "        # Find new indexes based on trial_len and sizes variables\n",
    "        start = end\n",
    "        new_elements = sum(trial_len[sum(sizes[0:tar]):sum(sizes[0:tar+1])])\n",
    "        end = end + new_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "# Import the whole data set\n",
    "# combine time bins into longer ones\n",
    "td_full = combine_time_bins(df, 3)\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_full = remove_low_firing_neurons(td_full, \"M1_spikes\",  5)\n",
    "td_full = remove_low_firing_neurons(td_full, \"PMd_spikes\", 5)\n",
    "\n",
    "# Get the signal from idx_go_cue\n",
    "df.idx_movement_on = df.idx_movement_on.astype(int)\n",
    "td_full = restrict_to_interval(td_full, start_point_name='idx_go_cue', end_point_name='idx_trial_end')\n",
    "\n",
    "\n",
    "td_full = smooth_signals(td_full, [\"M1_spikes\", \"PMd_spikes\"], std=0.05)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification preprocessing\n",
    "\n",
    "# Preprocessing\n",
    "# combine time bins into longer ones, e.g. group 3 time bins together\n",
    "td_class = combine_time_bins(df, 3)\n",
    "\n",
    "# Obtain only the interval between idx_target_on and idx_go_cue\n",
    "td_class = restrict_to_interval(td_class, start_point_name='idx_target_on', end_point_name='idx_go_cue')\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_class = remove_low_firing_neurons(td_class, \"M1_spikes\",  5)\n",
    "td_class = remove_low_firing_neurons(td_class, \"PMd_spikes\", 5)\n",
    "\n",
    "# total number of trials\n",
    "N = td_class.shape[0]\n",
    "\n",
    "#Number of M1_neurons\n",
    "N_M1 = td_class.M1_spikes[0].shape[1]\n",
    "#Number of PMd_neurons\n",
    "N_PMd = td_class.PMd_spikes[0].shape[1]\n",
    "\n",
    "M1_spikes = np.empty([N_M1,N])\n",
    "PMd_spikes = np.empty([N_PMd,N])\n",
    "y = np.empty([N,1])\n",
    "\n",
    "for i in range(N):\n",
    "    # Get the neuron spikes for a given trial in train data\n",
    "    M1_trial = np.transpose(td_class.M1_spikes[i])\n",
    "    PMd_trial = np.transpose(td_class.PMd_spikes[i])\n",
    "    \n",
    "    # Sum all the spikes in the given trial and save them\n",
    "    M1_spikes[:,i] = np.sum(M1_trial, axis=1)\n",
    "    PMd_spikes[:,i] = np.sum(PMd_trial, axis=1)\n",
    "    \n",
    "    # Get the label\n",
    "    y[i] = determine_angle(td_class.target_direction[i])\n",
    "    \n",
    "# Build a feature vector\n",
    "F_M1 = np.empty([N, N_M1])\n",
    "F_PMd = np.empty([N, N_PMd])\n",
    "for i in range(N):#in range(M1_spikes.shape[1]):\n",
    "    total_M1_spikes = np.sum(M1_spikes[:,i]);\n",
    "    total_PMd_spikes = np.sum(PMd_spikes[:,i])\n",
    "    \n",
    "    f_M1 = np.transpose(M1_spikes[:,i])/total_M1_spikes\n",
    "    f_PMd = np.transpose(PMd_spikes[:,i])/total_PMd_spikes\n",
    "    \n",
    "    # Store average firing rates\n",
    "    F_M1[i,:] = f_M1\n",
    "    F_PMd[i,:] = f_PMd\n",
    "    \n",
    "# Combine M1 and PMd features\n",
    "F_M1_PMd = np.concatenate((F_M1, F_PMd), axis = 1)\n",
    "\n",
    "# Split the data into test and train subsets\n",
    "split = int(0.8*N)\n",
    "\n",
    "y_train = y[0:split-1]\n",
    "y_test = y[split:]\n",
    "\n",
    "F_M1_PMd_train = F_M1_PMd[0:split-1,:]\n",
    "F_M1_PMd_test = F_M1_PMd[split:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000121B7745378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:61: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 value: -2.8809298297008303\n",
      "WARNING:tensorflow:5 out of the last 277 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000121B7B8A400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 279 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000121B9A83400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "R2 value: 0.9247391149016264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:61: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "## Make predictions\n",
    "N_trials = 740\n",
    "start = 0\n",
    "kinematics = [td_full.pos, td_full.vel]\n",
    "\n",
    "for (idx,output) in enumerate(kinematics):\n",
    "    predictions = []\n",
    "    y_valid_full = []\n",
    "    y_pred_full = []\n",
    "    for i in range(N_trials-int(0.8*N_trials)):\n",
    "        #end = start + trial - 1\n",
    "        #print(int(0.8*N_trials),i)\n",
    "        neural_data = td_full.M1_spikes[int(0.8*N_trials)+i]\n",
    "        y_valid = output[int(0.8*N_trials)+i]\n",
    "\n",
    "        class_prediction = y_test[i]\n",
    "        #class_prediction =sv_classifier.predict([F_M1_PMd_test[i]])\n",
    "        #print(class_prediction)\n",
    "        predictions.append(class_prediction)\n",
    "\n",
    "\n",
    "\n",
    "        # Preprocess data\n",
    "        bins_before=6 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        # Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "        # Function to get the covariate matrix that includes spike history from previous bins\n",
    "        X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "\n",
    "        # Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "        #Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "        X_flat_final=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "\n",
    "        X_flat_final = np.nan_to_num(X_flat_final)\n",
    "\n",
    "       # X_flat_final = (X_flat_final-x_flat_mean[int(class_prediction-1)])/x_flat_std[int(class_prediction-1)]\n",
    "\n",
    "       # y_valid = y_valid-label_means[int(class_prediction-1)]\n",
    "\n",
    "        # Avoid some errors\n",
    "        if X_flat_final.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        X = np.nan_to_num(X)\n",
    "        y_valid_predicted = LSTM_models[int(class_prediction-1)+8*idx].predict(X)\n",
    "\n",
    "\n",
    "        y_valid_full.append(y_valid)\n",
    "        y_pred_full.append(y_valid_predicted)\n",
    "\n",
    "        # Update the starting point of the data for next iteration of the loop\n",
    "        start = end + 1\n",
    "\n",
    "        #y_pred_plot = y_valid_predicted\n",
    "        #plt.plot(np.transpose(y_pred_plot[:,0]), np.transpose(y_pred_plot[:,1]))\n",
    "    \n",
    "    y_valid_full = np.array(y_valid_full)\n",
    "    y_pred_full = np.array(y_pred_full)\n",
    "\n",
    "\n",
    "    for i in range(y_valid_full.shape[0]):\n",
    "        if i == 0:\n",
    "            y_val = np.array(np.squeeze(y_valid_full[i]))\n",
    "            y_pred = np.array(np.squeeze(y_pred_full[i]))\n",
    "        else:\n",
    "            y_val = np.concatenate((y_val, np.squeeze(y_valid_full[i])), axis=0)\n",
    "            y_pred = np.concatenate((y_pred, np.squeeze(y_pred_full[i])), axis=0)\n",
    "\n",
    "    R2_vw = r2_score(y_val,y_pred, multioutput='variance_weighted')\n",
    "    print('R2 value:', R2_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
