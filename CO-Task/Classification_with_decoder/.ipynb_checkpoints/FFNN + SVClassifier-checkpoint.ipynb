{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyaldata import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "\n",
    "#Import hyperparameter optimization packages\n",
    "try:\n",
    "    from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "except ImportError:\n",
    "    print(\"\\nWARNING: hyperopt package is not installed. You will be unable to use section 5.\")\n",
    "    pass\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import DenseNNDecoder\n",
    "from Neural_Decoding.decoders import SVClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reassigns different angles into classes from 1 to 8\n",
    "# going anticlockwise starting from +x direction\n",
    "def determine_angle(angle):\n",
    "    if angle == 0:\n",
    "        return 1\n",
    "    elif angle == math.pi/4:\n",
    "        return 2\n",
    "    elif angle == math.pi/2:\n",
    "        return 3\n",
    "    elif angle == 3*math.pi/4:\n",
    "        return 4\n",
    "    elif angle == math.pi:\n",
    "        return 5\n",
    "    elif angle == -3*math.pi/4:\n",
    "        return 6\n",
    "    elif angle == -math.pi/2:\n",
    "        return 7\n",
    "    elif angle == -math.pi/4:\n",
    "        return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = '../raw_data/'\n",
    "fname = os.path.join(data_dir, \"Chewie_CO_CS_2016-10-14.mat\")\n",
    "\n",
    "# load TrialData .mat file into a DataFrame\n",
    "df = mat2dataframe(fname, shift_idx_fields=True)\n",
    "\n",
    "# Keep only successful trials\n",
    "df = select_trials(df, \"result == 'R'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "## Classification preprocessing\n",
    "\n",
    "# Preprocessing\n",
    "# combine time bins into longer ones, e.g. group 3 time bins together\n",
    "td_class = combine_time_bins(df, 3)\n",
    "\n",
    "# Obtain only the interval between idx_target_on and idx_go_cue\n",
    "td_class = restrict_to_interval(td_class, start_point_name='idx_target_on', end_point_name='idx_go_cue')\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_class = remove_low_firing_neurons(td_class, \"M1_spikes\",  5)\n",
    "td_class = remove_low_firing_neurons(td_class, \"PMd_spikes\", 5)\n",
    "\n",
    "# total number of trials\n",
    "N = td_class.shape[0]\n",
    "\n",
    "#Number of M1_neurons\n",
    "N_M1 = td_class.M1_spikes[0].shape[1]\n",
    "#Number of PMd_neurons\n",
    "N_PMd = td_class.PMd_spikes[0].shape[1]\n",
    "\n",
    "M1_spikes = np.empty([N_M1,N])\n",
    "PMd_spikes = np.empty([N_PMd,N])\n",
    "y = np.empty([N,1])\n",
    "\n",
    "for i in range(N):\n",
    "    # Get the neuron spikes for a given trial in train data\n",
    "    M1_trial = np.transpose(td_class.M1_spikes[i])\n",
    "    PMd_trial = np.transpose(td_class.PMd_spikes[i])\n",
    "    \n",
    "    # Sum all the spikes in the given trial and save them\n",
    "    M1_spikes[:,i] = np.sum(M1_trial, axis=1)\n",
    "    PMd_spikes[:,i] = np.sum(PMd_trial, axis=1)\n",
    "    \n",
    "    # Get the label\n",
    "    y[i] = determine_angle(td_class.target_direction[i])\n",
    "    \n",
    "# Build a feature vector\n",
    "F_M1 = np.empty([N, N_M1])\n",
    "F_PMd = np.empty([N, N_PMd])\n",
    "for i in range(N):#in range(M1_spikes.shape[1]):\n",
    "    total_M1_spikes = np.sum(M1_spikes[:,i]);\n",
    "    total_PMd_spikes = np.sum(PMd_spikes[:,i])\n",
    "    \n",
    "    f_M1 = np.transpose(M1_spikes[:,i])/total_M1_spikes\n",
    "    f_PMd = np.transpose(PMd_spikes[:,i])/total_PMd_spikes\n",
    "    \n",
    "    # Store average firing rates\n",
    "    F_M1[i,:] = f_M1\n",
    "    F_PMd[i,:] = f_PMd\n",
    "    \n",
    "# Combine M1 and PMd features\n",
    "F_M1_PMd = np.concatenate((F_M1, F_PMd), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train subsets\n",
    "split = int(0.8*N)\n",
    "\n",
    "y_train = y[0:split-1]\n",
    "y_test = y[split:]\n",
    "\n",
    "F_M1_PMd_train = F_M1_PMd[0:split-1,:]\n",
    "F_M1_PMd_test = F_M1_PMd[split:,:]\n",
    "\n",
    "\n",
    "## Train classifiers\n",
    "# Support vector classification\n",
    "sv_classifier = SVClassification()\n",
    "\n",
    "sv_classifier.fit(F_M1_PMd_train, np.squeeze(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|██████████▏                                        | 1/5 [00:00<00:02,  1.34trial/s, best loss: 47.07449618129339]\u001b[A\n",
      " 40%|████████████████████▍                              | 2/5 [00:03<00:06,  2.05s/trial, best loss: 11.92874929339955]\u001b[A\n",
      " 60%|██████████████████████████████▌                    | 3/5 [00:04<00:03,  1.64s/trial, best loss: 11.92874929339955]\u001b[A\n",
      " 80%|████████████████████████████████████████▊          | 4/5 [00:06<00:01,  1.47s/trial, best loss: 11.92874929339955]\u001b[A\n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.75s/trial, best loss: 9.737791695175348]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▌                                                                         | 1/8 [00:10<01:13, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:02<00:08,  2.06s/trial, best loss: -0.6555297737601651]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:03<00:05,  1.75s/trial, best loss: -0.6555297737601651]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:06<00:04,  2.10s/trial, best loss: -0.6555297737601651]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:15<00:05,  5.08s/trial, best loss: -0.6555297737601651]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:21<00:00,  4.29s/trial, best loss: -0.6555297737601651]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████                                                               | 2/8 [00:33<01:47, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:04<00:19,  4.93s/trial, best loss: -0.2963717227267783]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:09<00:14,  4.86s/trial, best loss: -0.2963717227267783]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:11<00:06,  3.43s/trial, best loss: -0.2963717227267783]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:17<00:04,  4.27s/trial, best loss: -0.2963717227267783]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.01s/trial, best loss: -0.2963717227267783]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:56<01:40, 20.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:01<00:05,  1.30s/trial, best loss: -0.6201126741845309]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:02<00:03,  1.16s/trial, best loss: -0.6677966597140033]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:04<00:02,  1.47s/trial, best loss: -0.6677966597140033]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:05<00:01,  1.31s/trial, best loss: -0.6677966597140033]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.46s/trial, best loss: -0.7312618583277997]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 4/8 [01:04<01:02, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|██████████▏                                        | 1/5 [00:02<00:11,  2.84s/trial, best loss: 8.846812300461245]\u001b[A\n",
      " 40%|████████████████████▍                              | 2/5 [00:04<00:06,  2.27s/trial, best loss: 8.846812300461245]\u001b[A\n",
      " 60%|██████████████████████████████▌                    | 3/5 [00:06<00:04,  2.27s/trial, best loss: 8.492721901624556]\u001b[A\n",
      " 80%|████████████████████████████████████████▊          | 4/5 [00:07<00:01,  1.76s/trial, best loss: 8.492721901624556]\u001b[A\n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.66s/trial, best loss: 8.492721901624556]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [01:24<00:51, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|██████████                                        | 1/5 [00:00<00:03,  1.28trial/s, best loss: 0.3801344081359928]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:04<00:07,  2.66s/trial, best loss: -0.2549645203426927]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 3/5 [00:05<00:03,  1.77s/trial, best loss: -0.39919681378563454]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:08<00:02,  2.13s/trial, best loss: -0.4561572630450661]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.10s/trial, best loss: -0.4561572630450661]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [01:36<00:30, 15.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▌                                      | 1/5 [00:04<00:19,  4.88s/trial, best loss: -0.39906487643251715]\u001b[A\n",
      " 40%|███████████████████▏                            | 2/5 [00:09<00:13,  4.66s/trial, best loss: -0.39906487643251715]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:18<00:13,  6.87s/trial, best loss: -0.4155730120567039]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:23<00:06,  6.12s/trial, best loss: -0.4155730120567039]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.09s/trial, best loss: -0.4155730120567039]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [02:06<00:19, 19.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:03<00:14,  3.60s/trial, best loss: -0.3872319374652566]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:04<00:06,  2.30s/trial, best loss: -0.5379320566601329]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:06<00:03,  1.96s/trial, best loss: -0.5379320566601329]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:16<00:05,  5.17s/trial, best loss: -0.5677950301872198]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.60s/trial, best loss: -0.5677950301872198]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [02:27<00:00, 18.48s/it]\n",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:00<00:03,  1.03trial/s, best loss: -0.5894689315887334]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:02<00:03,  1.11s/trial, best loss: -0.6613976925760454]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:03<00:02,  1.10s/trial, best loss: -0.6613976925760454]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:04<00:01,  1.19s/trial, best loss: -0.6613976925760454]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.76s/trial, best loss: -0.7108434003024156]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▌                                                                         | 1/8 [00:10<01:15, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:01<00:04,  1.18s/trial, best loss: -0.9438925259336388]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:08<00:14,  4.93s/trial, best loss: -0.9438925259336388]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:14<00:10,  5.34s/trial, best loss: -0.9438925259336388]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:17<00:04,  4.45s/trial, best loss: -0.9438925259336388]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.81s/trial, best loss: -0.9438925259336388]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████                                                               | 2/8 [00:30<01:36, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|██████████                                        | 1/5 [00:02<00:08,  2.19s/trial, best loss: -0.652520371636373]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:05<00:07,  2.60s/trial, best loss: -0.6617799687034007]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:09<00:07,  3.56s/trial, best loss: -0.6691726036945913]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:21<00:06,  6.81s/trial, best loss: -0.6691726036945913]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:24<00:00,  4.92s/trial, best loss: -0.6691726036945913]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:57<01:44, 20.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:02<00:09,  2.30s/trial, best loss: -0.8710349679871947]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:04<00:06,  2.19s/trial, best loss: -0.9012364631382119]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:06<00:04,  2.06s/trial, best loss: -0.9054024159917806]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:07<00:01,  1.81s/trial, best loss: -0.9054024159917806]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.75s/trial, best loss: -0.9054024159917806]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 4/8 [01:07<01:06, 16.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:03<00:12,  3.22s/trial, best loss: -0.5981198502778265]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:04<00:05,  1.79s/trial, best loss: -0.6095930643815364]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:06<00:04,  2.10s/trial, best loss: -0.6800973516975946]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:09<00:02,  2.57s/trial, best loss: -0.6800973516975946]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.12s/trial, best loss: -0.7039435327470476]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [01:25<00:52, 17.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:02<00:08,  2.10s/trial, best loss: -0.8894319363635703]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:03<00:04,  1.56s/trial, best loss: -0.8894319363635703]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:04<00:02,  1.45s/trial, best loss: -0.8894319363635703]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:06<00:01,  1.62s/trial, best loss: -0.8894319363635703]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.64s/trial, best loss: -0.8894319363635703]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [01:35<00:29, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:03<00:13,  3.40s/trial, best loss: -0.7791719587393315]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:06<00:10,  3.40s/trial, best loss: -0.7791719587393315]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:09<00:05,  2.96s/trial, best loss: -0.7791719587393315]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:10<00:02,  2.31s/trial, best loss: -0.7791719587393315]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.51s/trial, best loss: -0.7791719587393315]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [01:49<00:14, 14.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:03<00:15,  3.78s/trial, best loss: -0.8779999233390507]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:06<00:10,  3.42s/trial, best loss: -0.9147427130905266]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:11<00:07,  3.92s/trial, best loss: -0.9147427130905266]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:15<00:03,  3.90s/trial, best loss: -0.9147427130905266]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.74s/trial, best loss: -0.9147427130905266]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [02:10<00:00, 16.32s/it]\n"
     ]
    }
   ],
   "source": [
    "## Regression preprocessing\n",
    "folder='../preprocessed_data/'\n",
    "#ENTER THE FOLDER THAT YOUR DATA IS IN\n",
    "\n",
    "with open(folder+'individual_tar_data.pickle','rb') as f:\n",
    "    M1, M1_PMd,pos_binned,vels_binned,sizes,trial_len=pickle.load(f,encoding='latin1') #If using python 3\n",
    "\n",
    "neural_data = M1\n",
    "kinematics = [pos_binned, vels_binned] \n",
    "\n",
    "FFNN_models = []\n",
    "label_means = []\n",
    "x_flat_mean = []\n",
    "x_flat_std = []\n",
    "N_tar = 8\n",
    "for (idx,output) in enumerate(kinematics):\n",
    "    start = 0\n",
    "    end = sum(trial_len[0:sizes[0]])\n",
    "\n",
    "    # Loop over the data to obtain decoders for all 8 targets\n",
    "    for tar in tqdm(range(1, N_tar+1)): \n",
    "\n",
    "        # Preprocess data\n",
    "        bins_before=6 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        # Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "        # Function to get the covariate matrix that includes spike history from previous bins\n",
    "        X=get_spikes_with_history(neural_data[start:end],bins_before,bins_after,bins_current)\n",
    "\n",
    "        # Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "        #Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "        X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "        # Output covariates\n",
    "        #Set decoding output\n",
    "        y=output[start:end]\n",
    "\n",
    "        # Split into training / testing / validation sets\n",
    "        #Set what part of data should be part of the training/testing/validation sets\n",
    "        training_range=[0, 0.7]\n",
    "        testing_range=[0.7, 0.85]\n",
    "        valid_range=[0.85,1]\n",
    "\n",
    "        # Split data:\n",
    "        num_examples=X.shape[0]\n",
    "\n",
    "        #Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "        #This makes it so that the different sets don't include overlapping neural data\n",
    "        training_set=np.arange(int(np.round(training_range[0]*num_examples))+bins_before,int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "        testing_set=np.arange(int(np.round(testing_range[0]*num_examples))+bins_before,int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "        valid_set=np.arange(int(np.round(valid_range[0]*num_examples))+bins_before,int(np.round(valid_range[1]*num_examples))-bins_after)\n",
    "\n",
    "        #Get training data\n",
    "        X_train=X[training_set,:,:]\n",
    "        X_flat_train=X_flat[training_set,:]\n",
    "        y_train=y[training_set,:]\n",
    "\n",
    "        #Get testing data\n",
    "        X_test=X[testing_set,:,:]\n",
    "        X_flat_test=X_flat[testing_set,:]\n",
    "        y_test=y[testing_set,:]\n",
    "\n",
    "        #Get validation data\n",
    "        X_valid=X[valid_set,:,:]\n",
    "        X_flat_valid=X_flat[valid_set,:]\n",
    "        y_valid=y[valid_set,:]\n",
    "\n",
    "        # Process covariates\n",
    "        #Z-score \"X\" inputs. \n",
    "       # X_train_mean=np.nanmean(X_train,axis=0)\n",
    "       # X_train_std=np.nanstd(X_train,axis=0)\n",
    "       # X_train=(X_train-X_train_mean)/X_train_std\n",
    "        #X_test=(X_test-X_train_mean)/X_train_std\n",
    "       # X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "        #Z-score \"X_flat\" inputs. \n",
    "        #X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "       # X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "        #X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "        #X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "        #X_flat_valid=(X_flat_valid-X_flat_train_mean)/X_flat_train_std\n",
    "        \n",
    "        \n",
    "        #Zero-center outputs\n",
    "       # y_train_mean=np.mean(y_train,axis=0)\n",
    "       # y_train=y_train-y_train_mean\n",
    "        #y_test=y_test-y_train_mean\n",
    "       # y_valid=y_valid-y_train_mean\n",
    "\n",
    "        #Do optimization\n",
    "        # Define parameters for hyperoptimisation\n",
    "        def dnn_evaluate2(params):\n",
    "            #Put parameters in proper format\n",
    "            num_units=int(params['num_units'])\n",
    "            frac_dropout=float(params['frac_dropout'])\n",
    "            n_epochs=int(params['n_epochs'])\n",
    "            model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs) #Define model\n",
    "            model_dnn.fit(X_flat_train,y_train) #Fit model\n",
    "            y_valid_predicted_dnn=model_dnn.predict(X_flat_valid) #Get validation set predictions\n",
    "            return -np.mean(get_R2(y_valid,y_valid_predicted_dnn)) #Return -R2 value of validation set\n",
    "\n",
    "        #The range of values I'll look at for the parameter\n",
    "        #\"hp.quniform\" will allow us to look at integer (rather than continuously spaced) values.\n",
    "        #So for \"num_units\", we are looking at values between 50 and 700 by 10 (50,60,70,...700)\n",
    "        #\"hp.uniform\" looks at continuously spaced values\n",
    "        space = {\n",
    "            'frac_dropout': hp.uniform('frac_dropout', 0., 0.5),\n",
    "            'num_units': hp.quniform('num_units', 50,700,10),\n",
    "            'n_epochs': hp.quniform('n_epochs', 2,15,1),\n",
    "        }\n",
    "        #object that holds iteration results\n",
    "        trials = Trials()\n",
    "        \n",
    "        #Set the number of evaluations below (20 in this example)\n",
    "        hyperoptBest = fmin(dnn_evaluate2, space, algo=tpe.suggest, max_evals=5, trials=trials)        \n",
    "        \n",
    "\n",
    "        # Run decoder\n",
    "        #Declare model\n",
    "        model_dnn=DenseNNDecoder(units=int(hyperoptBest['num_units']),dropout=hyperoptBest['frac_dropout'],num_epochs=int(hyperoptBest['n_epochs']))\n",
    "\n",
    "        #Fit model\n",
    "        model_dnn.fit(X_flat_train,y_train)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        #R2_vw = r2_score(y_valid,y_valid_predicted_wf, multioutput='variance_weighted')\n",
    "\n",
    "        # Save the R2 value for a given neural data and kinematics\n",
    "        #R2[idx,tar-1] = R2_vw\n",
    "        FFNN_models.append(model_dnn)\n",
    "        #label_means.append(y_train_mean)\n",
    "        #x_flat_mean.append(X_flat_train_mean)\n",
    "        #x_flat_std.append(X_flat_train_std)\n",
    "        \n",
    "        # Find new indexes based on trial_len and sizes variables\n",
    "        start = end\n",
    "        new_elements = sum(trial_len[sum(sizes[0:tar]):sum(sizes[0:tar+1])])\n",
    "        end = end + new_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(FFNN_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "# Import the whole data set\n",
    "# combine time bins into longer ones\n",
    "td_full = combine_time_bins(df, 3)\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_full = remove_low_firing_neurons(td_full, \"M1_spikes\",  5)\n",
    "td_full = remove_low_firing_neurons(td_full, \"PMd_spikes\", 5)\n",
    "\n",
    "# Get the signal from idx_go_cue\n",
    "df.idx_movement_on = df.idx_movement_on.astype(int)\n",
    "td_full = restrict_to_interval(td_full, start_point_name='idx_go_cue', end_point_name='idx_trial_end')\n",
    "\n",
    "\n",
    "td_full = smooth_signals(td_full, [\"M1_spikes\", \"PMd_spikes\"], std=0.05)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_109 is incompatible with the layer: expected axis -1 of input shape to have value 441 but received input with shape (None, 7, 63)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-de4de9072a48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0my_valid_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFFNN_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_prediction\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\neural_decoding-0.1.2.dev0-py3.7.egg\\Neural_Decoding\\decoders.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X_flat_test)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \"\"\"\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[0my_test_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_flat_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_test_predicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m-> 2941\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[1;32m-> 3358\u001b[1;33m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3280\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_109 is incompatible with the layer: expected axis -1 of input shape to have value 441 but received input with shape (None, 7, 63)\n"
     ]
    }
   ],
   "source": [
    "## Make predictions\n",
    "N_trials = 740\n",
    "start = 0\n",
    "kinematics = [td_full.pos, td_full.vel]\n",
    "\n",
    "for (idx,output) in enumerate(kinematics):\n",
    "    predictions = []\n",
    "    y_valid_full = []\n",
    "    y_pred_full = []\n",
    "    for i in range(N_trials-int(0.8*N_trials)):\n",
    "        #end = start + trial - 1\n",
    "        #print(int(0.8*N_trials),i)\n",
    "        neural_data = td_full.M1_spikes[int(0.8*N_trials)+i]\n",
    "        y_valid = output[int(0.8*N_trials)+i]\n",
    "\n",
    "       # class_prediction = y_test[i]\n",
    "        class_prediction =sv_classifier.predict([F_M1_PMd_test[i]])\n",
    "        #print(class_prediction)\n",
    "        predictions.append(class_prediction)\n",
    "\n",
    "\n",
    "\n",
    "        # Preprocess data\n",
    "        bins_before=6 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        # Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "        # Function to get the covariate matrix that includes spike history from previous bins\n",
    "        X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "\n",
    "        # Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "        #Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "        #X_flat_final=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "\n",
    "        X_flat_final = np.nan_to_num(X_flat_final)\n",
    "\n",
    "       # X_flat_final = (X_flat_final-x_flat_mean[int(class_prediction-1)])/x_flat_std[int(class_prediction-1)]\n",
    "\n",
    "       # y_valid = y_valid-label_means[int(class_prediction-1)]\n",
    "\n",
    "        # Avoid some errors\n",
    "        if X_flat_final.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        y_valid_predicted = FFNN_models[int(class_prediction-1)+8*idx].predict(X_flat_final)\n",
    "\n",
    "\n",
    "        y_valid_full.append(y_valid)\n",
    "        y_pred_full.append(y_valid_predicted)\n",
    "\n",
    "        # Update the starting point of the data for next iteration of the loop\n",
    "        start = end + 1\n",
    "\n",
    "        #y_pred_plot = y_valid_predicted\n",
    "        #plt.plot(np.transpose(y_pred_plot[:,0]), np.transpose(y_pred_plot[:,1]))\n",
    "    \n",
    "    y_valid_full = np.array(y_valid_full)\n",
    "    y_pred_full = np.array(y_pred_full)\n",
    "\n",
    "\n",
    "    for i in range(y_valid_full.shape[0]):\n",
    "        if i == 0:\n",
    "            y_val = np.array(np.squeeze(y_valid_full[i]))\n",
    "            y_pred = np.array(np.squeeze(y_pred_full[i]))\n",
    "        else:\n",
    "            y_val = np.concatenate((y_val, np.squeeze(y_valid_full[i])), axis=0)\n",
    "            y_pred = np.concatenate((y_pred, np.squeeze(y_pred_full[i])), axis=0)\n",
    "\n",
    "    R2_vw = r2_score(y_val,y_pred, multioutput='variance_weighted')\n",
    "    print('R2 value:', R2_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
