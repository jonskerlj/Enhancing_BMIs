{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyaldata import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "\n",
    "#Import hyperparameter optimization packages\n",
    "try:\n",
    "    from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "except ImportError:\n",
    "    print(\"\\nWARNING: hyperopt package is not installed. You will be unable to use section 5.\")\n",
    "    pass\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import DenseNNDecoder\n",
    "from Neural_Decoding.decoders import SVClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reassigns different angles into classes from 1 to 8\n",
    "# going anticlockwise starting from +x direction\n",
    "def determine_angle(angle):\n",
    "    if angle == 0:\n",
    "        return 1\n",
    "    elif angle == math.pi/4:\n",
    "        return 2\n",
    "    elif angle == math.pi/2:\n",
    "        return 3\n",
    "    elif angle == 3*math.pi/4:\n",
    "        return 4\n",
    "    elif angle == math.pi:\n",
    "        return 5\n",
    "    elif angle == -3*math.pi/4:\n",
    "        return 6\n",
    "    elif angle == -math.pi/2:\n",
    "        return 7\n",
    "    elif angle == -math.pi/4:\n",
    "        return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = '../raw_data/'\n",
    "fname = os.path.join(data_dir, \"Chewie_CO_CS_2016-10-14.mat\")\n",
    "\n",
    "# load TrialData .mat file into a DataFrame\n",
    "df = mat2dataframe(fname, shift_idx_fields=True)\n",
    "\n",
    "# Keep only successful trials\n",
    "df = select_trials(df, \"result == 'R'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "## Classification preprocessing\n",
    "\n",
    "# Preprocessing\n",
    "# combine time bins into longer ones, e.g. group 3 time bins together\n",
    "td_class = combine_time_bins(df, 3)\n",
    "\n",
    "# Obtain only the interval between idx_target_on and idx_go_cue\n",
    "td_class = restrict_to_interval(td_class, start_point_name='idx_target_on', end_point_name='idx_go_cue')\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_class = remove_low_firing_neurons(td_class, \"M1_spikes\",  5)\n",
    "td_class = remove_low_firing_neurons(td_class, \"PMd_spikes\", 5)\n",
    "\n",
    "# total number of trials\n",
    "N = td_class.shape[0]\n",
    "\n",
    "#Number of M1_neurons\n",
    "N_M1 = td_class.M1_spikes[0].shape[1]\n",
    "#Number of PMd_neurons\n",
    "N_PMd = td_class.PMd_spikes[0].shape[1]\n",
    "\n",
    "M1_spikes = np.empty([N_M1,N])\n",
    "PMd_spikes = np.empty([N_PMd,N])\n",
    "y = np.empty([N,1])\n",
    "\n",
    "for i in range(N):\n",
    "    # Get the neuron spikes for a given trial in train data\n",
    "    M1_trial = np.transpose(td_class.M1_spikes[i])\n",
    "    PMd_trial = np.transpose(td_class.PMd_spikes[i])\n",
    "    \n",
    "    # Sum all the spikes in the given trial and save them\n",
    "    M1_spikes[:,i] = np.sum(M1_trial, axis=1)\n",
    "    PMd_spikes[:,i] = np.sum(PMd_trial, axis=1)\n",
    "    \n",
    "    # Get the label\n",
    "    y[i] = determine_angle(td_class.target_direction[i])\n",
    "    \n",
    "# Build a feature vector\n",
    "F_M1 = np.empty([N, N_M1])\n",
    "F_PMd = np.empty([N, N_PMd])\n",
    "for i in range(N):#in range(M1_spikes.shape[1]):\n",
    "    total_M1_spikes = np.sum(M1_spikes[:,i]);\n",
    "    total_PMd_spikes = np.sum(PMd_spikes[:,i])\n",
    "    \n",
    "    f_M1 = np.transpose(M1_spikes[:,i])/total_M1_spikes\n",
    "    f_PMd = np.transpose(PMd_spikes[:,i])/total_PMd_spikes\n",
    "    \n",
    "    # Store average firing rates\n",
    "    F_M1[i,:] = f_M1\n",
    "    F_PMd[i,:] = f_PMd\n",
    "    \n",
    "# Combine M1 and PMd features\n",
    "F_M1_PMd = np.concatenate((F_M1, F_PMd), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train subsets\n",
    "split = int(0.8*N)\n",
    "\n",
    "y_train = y[0:split-1]\n",
    "y_test = y[split:]\n",
    "\n",
    "F_M1_PMd_train = F_M1_PMd[0:split-1,:]\n",
    "F_M1_PMd_test = F_M1_PMd[split:,:]\n",
    "\n",
    "\n",
    "## Train classifiers\n",
    "# Support vector classification\n",
    "sv_classifier = SVClassification()\n",
    "\n",
    "sv_classifier.fit(F_M1_PMd_train, np.squeeze(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|██████████▏                                        | 1/5 [00:04<00:17,  4.38s/trial, best loss: 32.06718119995789]\u001b[A\n",
      " 40%|████████████████████                              | 2/5 [00:06<00:08,  2.77s/trial, best loss: 13.419065668887036]\u001b[A\n",
      " 60%|██████████████████████████████                    | 3/5 [00:09<00:06,  3.27s/trial, best loss: 13.419065668887036]\u001b[A\n",
      " 80%|████████████████████████████████████████          | 4/5 [00:11<00:02,  2.72s/trial, best loss: 13.419065668887036]\u001b[A\n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.00s/trial, best loss: 12.331201731900205]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▌                                                                         | 1/8 [00:17<01:59, 17.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:01<00:04,  1.14s/trial, best loss: -0.6708395181072468]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:03<00:04,  1.61s/trial, best loss: -0.6708395181072468]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:04<00:03,  1.61s/trial, best loss: -0.6708395181072468]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:06<00:01,  1.68s/trial, best loss: -0.6708395181072468]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.55s/trial, best loss: -0.7293635257071317]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████                                                               | 2/8 [00:25<01:13, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▌                                      | 1/5 [00:02<00:08,  2.17s/trial, best loss: -0.05654064999949354]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:04<00:06,  2.16s/trial, best loss: -0.2040954443972941]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:07<00:05,  2.67s/trial, best loss: -0.2040954443972941]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 4/5 [00:11<00:03,  3.16s/trial, best loss: -0.24860357984791925]\u001b[A\n",
      "100%|████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.50s/trial, best loss: -0.24860357984791925]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:40<01:07, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:02<00:08,  2.23s/trial, best loss: -0.7198404925020503]\u001b[A\n",
      " 40%|████████████████████                              | 2/5 [00:06<00:11,  3.71s/trial, best loss: -0.845317742963059]\u001b[A\n",
      " 60%|██████████████████████████████                    | 3/5 [00:09<00:05,  2.97s/trial, best loss: -0.845317742963059]\u001b[A\n",
      " 80%|████████████████████████████████████████          | 4/5 [00:10<00:02,  2.37s/trial, best loss: -0.845317742963059]\u001b[A\n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.48s/trial, best loss: -0.845317742963059]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 4/8 [00:56<00:57, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|██████████▍                                         | 1/5 [00:03<00:12,  3.05s/trial, best loss: 5.88063065571507]\u001b[A\n",
      " 40%|████████████████████▊                               | 2/5 [00:04<00:06,  2.13s/trial, best loss: 5.88063065571507]\u001b[A\n",
      " 60%|███████████████████████████████▏                    | 3/5 [00:12<00:09,  4.82s/trial, best loss: 5.88063065571507]\u001b[A\n",
      " 80%|█████████████████████████████████████████▌          | 4/5 [00:16<00:04,  4.49s/trial, best loss: 5.88063065571507]\u001b[A\n",
      "100%|████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.08s/trial, best loss: 5.79950490648189]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [01:19<00:51, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|██████████                                        | 1/5 [00:02<00:10,  2.57s/trial, best loss: -0.549408942915103]\u001b[A\n",
      " 40%|████████████████████                              | 2/5 [00:04<00:05,  1.95s/trial, best loss: -0.549408942915103]\u001b[A\n",
      " 60%|██████████████████████████████                    | 3/5 [00:06<00:04,  2.29s/trial, best loss: -0.549408942915103]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:07<00:01,  1.86s/trial, best loss: -0.6181951674225369]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.87s/trial, best loss: -0.6181951674225369]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [01:29<00:29, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:02<00:08,  2.15s/trial, best loss: -0.3048546792053359]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:07<00:11,  3.91s/trial, best loss: -0.3048546792053359]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:08<00:04,  2.49s/trial, best loss: -0.3048546792053359]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 4/5 [00:15<00:04,  4.24s/trial, best loss: -0.48224208824424997]\u001b[A\n",
      "100%|████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.14s/trial, best loss: -0.48224208824424997]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [01:53<00:17, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:04<00:18,  4.67s/trial, best loss: -0.5048357672096389]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:15<00:24,  8.04s/trial, best loss: -0.6477720930668012]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:18<00:12,  6.06s/trial, best loss: -0.6598150668781846]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:29<00:07,  7.80s/trial, best loss: -0.6598150668781846]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:34<00:00,  6.81s/trial, best loss: -0.6598150668781846]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [02:29<00:00, 18.72s/it]\n",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:00<00:03,  1.02trial/s, best loss: -0.6147856050258846]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:07<00:13,  4.47s/trial, best loss: -0.6495828094294221]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:09<00:06,  3.26s/trial, best loss: -0.6662656505656279]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:10<00:02,  2.43s/trial, best loss: -0.6662656505656279]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.75s/trial, best loss: -0.6662656505656279]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▌                                                                         | 1/8 [00:14<01:43, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:04<00:17,  4.40s/trial, best loss: -0.9316034430185265]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:07<00:10,  3.61s/trial, best loss: -0.9318937093707057]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:08<00:04,  2.36s/trial, best loss: -0.9349070814833826]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:10<00:02,  2.09s/trial, best loss: -0.9349070814833826]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.27s/trial, best loss: -0.9349070814833826]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████                                                               | 2/8 [00:31<01:36, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:09<00:37,  9.29s/trial, best loss: -0.6542354866885196]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:14<00:21,  7.09s/trial, best loss: -0.6952745543708798]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:17<00:09,  4.84s/trial, best loss: -0.6952745543708798]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:18<00:03,  3.55s/trial, best loss: -0.6952745543708798]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.07s/trial, best loss: -0.6952745543708798]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:54<01:35, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:01<00:07,  1.87s/trial, best loss: -0.9051296439030643]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:02<00:04,  1.34s/trial, best loss: -0.9051296439030643]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:03<00:02,  1.09s/trial, best loss: -0.9142474287853732]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:04<00:00,  1.05trial/s, best loss: -0.9142474287853732]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.49s/trial, best loss: -0.9142474287853732]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 4/8 [01:07<01:07, 16.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:03<00:13,  3.48s/trial, best loss: -0.6812945061822844]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:08<00:12,  4.30s/trial, best loss: -0.6841581898050565]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:15<00:10,  5.42s/trial, best loss: -0.6841581898050565]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:17<00:04,  4.30s/trial, best loss: -0.6841581898050565]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:21<00:00,  4.38s/trial, best loss: -0.6841581898050565]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [01:31<00:58, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:01<00:04,  1.10s/trial, best loss: -0.8595205207151153]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:03<00:06,  2.01s/trial, best loss: -0.8595205207151153]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:05<00:03,  1.84s/trial, best loss: -0.8595205207151153]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:07<00:01,  1.91s/trial, best loss: -0.8742345407678394]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.75s/trial, best loss: -0.8742345407678394]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████                     | 6/8 [01:41<00:32, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:00<00:03,  1.13trial/s, best loss: -0.6487795262664535]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:04<00:07,  2.34s/trial, best loss: -0.7616420953951121]\u001b[A\n",
      " 60%|██████████████████████████████                    | 3/5 [00:07<00:05,  2.62s/trial, best loss: -0.766237290521411]\u001b[A\n",
      " 80%|███████████████████████████████████████▏         | 4/5 [00:09<00:02,  2.36s/trial, best loss: -0.7898069876105684]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.51s/trial, best loss: -0.7898069876105684]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [01:55<00:15, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      " 20%|█████████▊                                       | 1/5 [00:13<00:53, 13.44s/trial, best loss: -0.9023084756443558]\u001b[A\n",
      " 40%|███████████████████▌                             | 2/5 [00:14<00:19,  6.39s/trial, best loss: -0.9023084756443558]\u001b[A\n",
      " 60%|█████████████████████████████▍                   | 3/5 [00:17<00:09,  4.68s/trial, best loss: -0.9023084756443558]\u001b[A\n",
      " 80%|████████████████████████████████████████          | 4/5 [00:19<00:03,  3.80s/trial, best loss: -0.914892553153908]\u001b[A\n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:21<00:00,  4.32s/trial, best loss: -0.914892553153908]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [02:18<00:00, 17.28s/it]\n"
     ]
    }
   ],
   "source": [
    "## Regression preprocessing\n",
    "folder='../preprocessed_data/'\n",
    "#ENTER THE FOLDER THAT YOUR DATA IS IN\n",
    "\n",
    "with open(folder+'individual_tar_data.pickle','rb') as f:\n",
    "    M1, M1_PMd,pos_binned,vels_binned,sizes,trial_len=pickle.load(f,encoding='latin1') #If using python 3\n",
    "\n",
    "neural_data = M1\n",
    "kinematics = [pos_binned, vels_binned] \n",
    "\n",
    "FFNN_models = []\n",
    "label_means = []\n",
    "x_flat_mean = []\n",
    "x_flat_std = []\n",
    "N_tar = 8\n",
    "for (idx,output) in enumerate(kinematics):\n",
    "    start = 0\n",
    "    end = sum(trial_len[0:sizes[0]])\n",
    "\n",
    "    # Loop over the data to obtain decoders for all 8 targets\n",
    "    for tar in tqdm(range(1, N_tar+1)): \n",
    "\n",
    "        # Preprocess data\n",
    "        bins_before=6 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        # Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "        # Function to get the covariate matrix that includes spike history from previous bins\n",
    "        X=get_spikes_with_history(neural_data[start:end],bins_before,bins_after,bins_current)\n",
    "\n",
    "        # Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "        #Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "        X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "        # Output covariates\n",
    "        #Set decoding output\n",
    "        y=output[start:end]\n",
    "\n",
    "        # Split into training / testing / validation sets\n",
    "        #Set what part of data should be part of the training/testing/validation sets\n",
    "        training_range=[0, 0.7]\n",
    "        testing_range=[0.7, 0.85]\n",
    "        valid_range=[0.85,1]\n",
    "\n",
    "        # Split data:\n",
    "        num_examples=X.shape[0]\n",
    "\n",
    "        #Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "        #This makes it so that the different sets don't include overlapping neural data\n",
    "        training_set=np.arange(int(np.round(training_range[0]*num_examples))+bins_before,int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "        testing_set=np.arange(int(np.round(testing_range[0]*num_examples))+bins_before,int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "        valid_set=np.arange(int(np.round(valid_range[0]*num_examples))+bins_before,int(np.round(valid_range[1]*num_examples))-bins_after)\n",
    "\n",
    "        #Get training data\n",
    "        X_train=X[training_set,:,:]\n",
    "        X_flat_train=X_flat[training_set,:]\n",
    "        y_train=y[training_set,:]\n",
    "\n",
    "        #Get testing data\n",
    "        X_test=X[testing_set,:,:]\n",
    "        X_flat_test=X_flat[testing_set,:]\n",
    "        y_test=y[testing_set,:]\n",
    "\n",
    "        #Get validation data\n",
    "        X_valid=X[valid_set,:,:]\n",
    "        X_flat_valid=X_flat[valid_set,:]\n",
    "        y_valid=y[valid_set,:]\n",
    "\n",
    "        # Process covariates\n",
    "        #Z-score \"X\" inputs. \n",
    "       # X_train_mean=np.nanmean(X_train,axis=0)\n",
    "       # X_train_std=np.nanstd(X_train,axis=0)\n",
    "       # X_train=(X_train-X_train_mean)/X_train_std\n",
    "        #X_test=(X_test-X_train_mean)/X_train_std\n",
    "       # X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "        #Z-score \"X_flat\" inputs. \n",
    "        #X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "       # X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "        #X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "        #X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "        #X_flat_valid=(X_flat_valid-X_flat_train_mean)/X_flat_train_std\n",
    "        \n",
    "        \n",
    "        #Zero-center outputs\n",
    "       # y_train_mean=np.mean(y_train,axis=0)\n",
    "       # y_train=y_train-y_train_mean\n",
    "        #y_test=y_test-y_train_mean\n",
    "       # y_valid=y_valid-y_train_mean\n",
    "\n",
    "        #Do optimization\n",
    "        # Define parameters for hyperoptimisation\n",
    "        def dnn_evaluate2(params):\n",
    "            #Put parameters in proper format\n",
    "            num_units=int(params['num_units'])\n",
    "            frac_dropout=float(params['frac_dropout'])\n",
    "            n_epochs=int(params['n_epochs'])\n",
    "            model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs) #Define model\n",
    "            model_dnn.fit(X_flat_train,y_train) #Fit model\n",
    "            y_valid_predicted_dnn=model_dnn.predict(X_flat_valid) #Get validation set predictions\n",
    "            return -np.mean(get_R2(y_valid,y_valid_predicted_dnn)) #Return -R2 value of validation set\n",
    "\n",
    "        #The range of values I'll look at for the parameter\n",
    "        #\"hp.quniform\" will allow us to look at integer (rather than continuously spaced) values.\n",
    "        #So for \"num_units\", we are looking at values between 50 and 700 by 10 (50,60,70,...700)\n",
    "        #\"hp.uniform\" looks at continuously spaced values\n",
    "        space = {\n",
    "            'frac_dropout': hp.uniform('frac_dropout', 0., 0.5),\n",
    "            'num_units': hp.quniform('num_units', 50,700,10),\n",
    "            'n_epochs': hp.quniform('n_epochs', 2,15,1),\n",
    "        }\n",
    "        #object that holds iteration results\n",
    "        trials = Trials()\n",
    "        \n",
    "        #Set the number of evaluations below (20 in this example)\n",
    "        hyperoptBest = fmin(dnn_evaluate2, space, algo=tpe.suggest, max_evals=5, trials=trials)        \n",
    "        \n",
    "\n",
    "        # Run decoder\n",
    "        #Declare model\n",
    "        model_dnn=DenseNNDecoder(units=int(hyperoptBest['num_units']),dropout=hyperoptBest['frac_dropout'],num_epochs=int(hyperoptBest['n_epochs']))\n",
    "\n",
    "        #Fit model\n",
    "        model_dnn.fit(X_flat_train,y_train)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        #R2_vw = r2_score(y_valid,y_valid_predicted_wf, multioutput='variance_weighted')\n",
    "\n",
    "        # Save the R2 value for a given neural data and kinematics\n",
    "        #R2[idx,tar-1] = R2_vw\n",
    "        FFNN_models.append(model_dnn)\n",
    "        #label_means.append(y_train_mean)\n",
    "        #x_flat_mean.append(X_flat_train_mean)\n",
    "        #x_flat_std.append(X_flat_train_std)\n",
    "        \n",
    "        # Find new indexes based on trial_len and sizes variables\n",
    "        start = end\n",
    "        new_elements = sum(trial_len[sum(sizes[0:tar]):sum(sizes[0:tar+1])])\n",
    "        end = end + new_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(FFNN_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "# Import the whole data set\n",
    "# combine time bins into longer ones\n",
    "td_full = combine_time_bins(df, 3)\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_full = remove_low_firing_neurons(td_full, \"M1_spikes\",  5)\n",
    "td_full = remove_low_firing_neurons(td_full, \"PMd_spikes\", 5)\n",
    "\n",
    "# Get the signal from idx_go_cue\n",
    "df.idx_movement_on = df.idx_movement_on.astype(int)\n",
    "td_full = restrict_to_interval(td_full, start_point_name='idx_go_cue', end_point_name='idx_trial_end')\n",
    "\n",
    "\n",
    "td_full = smooth_signals(td_full, [\"M1_spikes\", \"PMd_spikes\"], std=0.05)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\documents\\biomedical engineering - ic\\4. letnik\\final year project\\python\\pyaldata\\pyaldata\\tools.py:979: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "## Classification preprocessing\n",
    "\n",
    "# Preprocessing\n",
    "# combine time bins into longer ones, e.g. group 3 time bins together\n",
    "td_class = combine_time_bins(df, 3)\n",
    "\n",
    "# Obtain only the interval between idx_target_on and idx_go_cue\n",
    "td_class = restrict_to_interval(td_class, start_point_name='idx_target_on', end_point_name='idx_go_cue')\n",
    "\n",
    "# Remove low-firing neurons\n",
    "td_class = remove_low_firing_neurons(td_class, \"M1_spikes\",  5)\n",
    "td_class = remove_low_firing_neurons(td_class, \"PMd_spikes\", 5)\n",
    "\n",
    "# total number of trials\n",
    "N = td_class.shape[0]\n",
    "\n",
    "#Number of M1_neurons\n",
    "N_M1 = td_class.M1_spikes[0].shape[1]\n",
    "#Number of PMd_neurons\n",
    "N_PMd = td_class.PMd_spikes[0].shape[1]\n",
    "\n",
    "M1_spikes = np.empty([N_M1,N])\n",
    "PMd_spikes = np.empty([N_PMd,N])\n",
    "y = np.empty([N,1])\n",
    "\n",
    "for i in range(N):\n",
    "    # Get the neuron spikes for a given trial in train data\n",
    "    M1_trial = np.transpose(td_class.M1_spikes[i])\n",
    "    PMd_trial = np.transpose(td_class.PMd_spikes[i])\n",
    "    \n",
    "    # Sum all the spikes in the given trial and save them\n",
    "    M1_spikes[:,i] = np.sum(M1_trial, axis=1)\n",
    "    PMd_spikes[:,i] = np.sum(PMd_trial, axis=1)\n",
    "    \n",
    "    # Get the label\n",
    "    y[i] = determine_angle(td_class.target_direction[i])\n",
    "    \n",
    "# Build a feature vector\n",
    "F_M1 = np.empty([N, N_M1])\n",
    "F_PMd = np.empty([N, N_PMd])\n",
    "for i in range(N):#in range(M1_spikes.shape[1]):\n",
    "    total_M1_spikes = np.sum(M1_spikes[:,i]);\n",
    "    total_PMd_spikes = np.sum(PMd_spikes[:,i])\n",
    "    \n",
    "    f_M1 = np.transpose(M1_spikes[:,i])/total_M1_spikes\n",
    "    f_PMd = np.transpose(PMd_spikes[:,i])/total_PMd_spikes\n",
    "    \n",
    "    # Store average firing rates\n",
    "    F_M1[i,:] = f_M1\n",
    "    F_PMd[i,:] = f_PMd\n",
    "    \n",
    "# Combine M1 and PMd features\n",
    "F_M1_PMd = np.concatenate((F_M1, F_PMd), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train subsets\n",
    "split = int(0.8*N)\n",
    "\n",
    "y_train = y[0:split-1]\n",
    "y_test = y[split:]\n",
    "\n",
    "F_M1_PMd_train = F_M1_PMd[0:split-1,:]\n",
    "F_M1_PMd_test = F_M1_PMd[split:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 value: -3.14937390638847\n",
      "R2 value: 0.9363399320004218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\jon\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "## Make predictions\n",
    "N_trials = 740\n",
    "start = 0\n",
    "kinematics = [td_full.pos, td_full.vel]\n",
    "\n",
    "for (idx,output) in enumerate(kinematics):\n",
    "    predictions = []\n",
    "    y_valid_full = []\n",
    "    y_pred_full = []\n",
    "    for i in range(N_trials-int(0.8*N_trials)):\n",
    "        #end = start + trial - 1\n",
    "        #print(int(0.8*N_trials),i)\n",
    "        neural_data = td_full.M1_spikes[int(0.8*N_trials)+i]\n",
    "        y_valid = output[int(0.8*N_trials)+i]\n",
    "\n",
    "        class_prediction = y_test[i]\n",
    "        #class_prediction =sv_classifier.predict([F_M1_PMd_test[i]])\n",
    "        #print(class_prediction)\n",
    "        predictions.append(class_prediction)\n",
    "\n",
    "\n",
    "\n",
    "        # Preprocess data\n",
    "        bins_before=6 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        # Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "        # Function to get the covariate matrix that includes spike history from previous bins\n",
    "        X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "\n",
    "        # Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "        #Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "        X_flat_final=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "\n",
    "\n",
    "        X_flat_final = np.nan_to_num(X_flat_final)\n",
    "\n",
    "       # X_flat_final = (X_flat_final-x_flat_mean[int(class_prediction-1)])/x_flat_std[int(class_prediction-1)]\n",
    "\n",
    "       # y_valid = y_valid-label_means[int(class_prediction-1)]\n",
    "\n",
    "        # Avoid some errors\n",
    "        if X_flat_final.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        y_valid_predicted = FFNN_models[int(class_prediction-1)+8*idx].predict(X_flat_final)\n",
    "\n",
    "\n",
    "        y_valid_full.append(y_valid)\n",
    "        y_pred_full.append(y_valid_predicted)\n",
    "\n",
    "        # Update the starting point of the data for next iteration of the loop\n",
    "        start = end + 1\n",
    "\n",
    "        #y_pred_plot = y_valid_predicted\n",
    "        #plt.plot(np.transpose(y_pred_plot[:,0]), np.transpose(y_pred_plot[:,1]))\n",
    "    \n",
    "    y_valid_full = np.array(y_valid_full)\n",
    "    y_pred_full = np.array(y_pred_full)\n",
    "\n",
    "\n",
    "    for i in range(y_valid_full.shape[0]):\n",
    "        if i == 0:\n",
    "            y_val = np.array(np.squeeze(y_valid_full[i]))\n",
    "            y_pred = np.array(np.squeeze(y_pred_full[i]))\n",
    "        else:\n",
    "            y_val = np.concatenate((y_val, np.squeeze(y_valid_full[i])), axis=0)\n",
    "            y_pred = np.concatenate((y_pred, np.squeeze(y_pred_full[i])), axis=0)\n",
    "\n",
    "    R2_vw = r2_score(y_val,y_pred, multioutput='variance_weighted')\n",
    "    print('R2 value:', R2_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
